{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8zOon65tQHG"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbZgdIQItUF1",
        "outputId": "81f98e9c-d071-47c2-ed50-2f3e0b58fe40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 1.792s\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjK1LnuvtZit",
        "outputId": "8c29c614-4ae6-41e9-d55b-ac08d6559f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.106.136.242\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8tlyl5xPjtP"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Loading data\n",
        "ba_analytics_df = pd.read_excel('data sets BA analytics.xlsx')\n",
        "data = pd.read_excel('subtests.xlsx')  # Replace with your skill sets file path\n",
        "data['Skills Assessed'] = data['Skills Assessed'].fillna('').astype(str)\n",
        "\n",
        "#make list with skills and corresponding weights\n",
        "def extract_skills_and_weights(text):\n",
        "    skills_weights = text.split('\\n')\n",
        "    skills_weights_list = []\n",
        "    for sw in skills_weights:\n",
        "        match = re.search(r'-\\s*([\\d.]+)%', sw)\n",
        "        if match:\n",
        "            weight = float(match.group(1)) / 100.0\n",
        "            skill = sw[:match.start()].strip()\n",
        "            skills_weights_list.append((skill, weight))\n",
        "    return skills_weights_list\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "def get_embeddings(text_list):\n",
        "    inputs = tokenizer(text_list, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "    return embeddings\n",
        "\n",
        "def match_skills_with_tests(skill_set, skill_offset, test_names, skills_embeddings, tests_embeddings):\n",
        "    matches = []\n",
        "    for i, (skill, weight) in enumerate(skill_set):\n",
        "        skill_embedding = torch.tensor(skills_embeddings[skill_offset + i])\n",
        "        similarities = []\n",
        "        for j, test_embedding in enumerate(tests_embeddings):\n",
        "            test_embedding = torch.tensor(test_embedding)\n",
        "            similarity = torch.nn.functional.cosine_similarity(skill_embedding.unsqueeze(0), test_embedding.unsqueeze(0), dim=1).item()\n",
        "            similarities.append((test_names[j], similarity))\n",
        "        matches.append(sorted(similarities, key=lambda x: x[1], reverse=True))\n",
        "    return matches\n",
        "\n",
        "def select_suitable_tests(skill_set, matches, data, min_time=40, max_time=60):\n",
        "    weighted_matches = []\n",
        "    for i, skill in enumerate(skill_set):\n",
        "        skill_weight = skill[1]\n",
        "        weighted_scores = [(match[0], match[1] * skill_weight if match[1] > 0 else 0) for match in matches[i]]\n",
        "        weighted_matches.append(sorted(weighted_scores, key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    final_tests = {}\n",
        "    for test, score in sum(weighted_matches, []):\n",
        "        if test in final_tests:\n",
        "            final_tests[test] += score\n",
        "        else:\n",
        "            final_tests[test] = score\n",
        "    sorted_tests = sorted(final_tests.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    #Time constraints\n",
        "    selected_tests = []\n",
        "    total_time = 0\n",
        "\n",
        "    for test, score in sorted_tests:\n",
        "        test_time = data[data.apply(lambda row: f\"{row['Test Type']} - {row['Subcategory']}\", axis=1) == test]['Time'].values[0]\n",
        "        try:\n",
        "            test_time = int(test_time)\n",
        "        except ValueError:\n",
        "            test_time = 0\n",
        "\n",
        "        if total_time + test_time <= max_time:\n",
        "            selected_tests.append((test, score))\n",
        "            total_time += test_time\n",
        "        if total_time >= min_time:\n",
        "            break\n",
        "\n",
        "\n",
        "    if total_time < min_time:\n",
        "        for test, score in sorted_tests[len(selected_tests):]:\n",
        "            test_time = data[data.apply(lambda row: f\"{row['Test Type']} - {row['Subcategory']}\", axis=1) == test]['Time'].values[0]\n",
        "            try:\n",
        "                test_time = int(test_time)\n",
        "            except ValueError:\n",
        "                test_time = 0\n",
        "\n",
        "            selected_tests.append((test, score))\n",
        "            total_time += test_time\n",
        "            if total_time >= max_time:\n",
        "                break\n",
        "\n",
        "    return selected_tests, total_time, sorted_tests\n",
        "\n",
        "def find_similar_tests(selected_tests, tests_embeddings, threshold=0.9):\n",
        "    similar_tests = {}\n",
        "    selected_test_names = [test[0] for test in selected_tests]\n",
        "    selected_test_embeddings = [tests_embeddings[test_names.index(test[0])] for test in selected_tests]\n",
        "\n",
        "    for i, (test1, _) in enumerate(selected_tests):\n",
        "        similar_tests[test1] = []\n",
        "        embedding1 = torch.tensor(selected_test_embeddings[i])\n",
        "        for j, (test2, _) in enumerate(selected_tests):\n",
        "            if i != j:\n",
        "                embedding2 = torch.tensor(selected_test_embeddings[j])\n",
        "                similarity = torch.nn.functional.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0), dim=1).item()\n",
        "                if similarity > threshold:\n",
        "                    similar_tests[test1].append((test2, similarity))\n",
        "    return similar_tests\n",
        "\n",
        "def remove_lowest_score_test(selected_tests, similar_tests, data):\n",
        "    test_to_remove = None\n",
        "    for test1, similars in similar_tests.items():\n",
        "        if len(similars) > 0:\n",
        "            similar_test = max(similars, key=lambda x: x[1])\n",
        "            test1_score = next(score for test, score in selected_tests if test == test1)\n",
        "            similar_test_score = next(score for test, score in selected_tests if test == similar_test[0])\n",
        "            if test1_score < similar_test_score:\n",
        "                test_to_remove = test1\n",
        "            else:\n",
        "                test_to_remove = similar_test[0]\n",
        "            break\n",
        "\n",
        "    if not test_to_remove:\n",
        "        test_to_remove = min(selected_tests, key=lambda x: x[1])[0]\n",
        "\n",
        "    test_time = data[data.apply(lambda row: f\"{row['Test Type']} - {row['Subcategory']}\", axis=1) == test_to_remove]['Time'].values[0]\n",
        "    return test_to_remove, int(test_time)\n",
        "\n",
        "\n",
        "skill_sets = []\n",
        "for index, row in ba_analytics_df.iterrows():\n",
        "    skill_set = extract_skills_and_weights(row['Constructs and Weights (Step 3)'])\n",
        "    skill_sets.append(skill_set)\n",
        "\n",
        "for skill_set in skill_sets:\n",
        "    for i, (skill, weight) in enumerate(skill_set):\n",
        "        skill_set[i] = (preprocess_text(skill), weight)\n",
        "\n",
        "data['Combined'] = (data['Description'] + \" \" + data['Skills Assessed']).apply(preprocess_text)\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "skills_text = [skill for skill_set in skill_sets for skill, weight in skill_set]\n",
        "tests_text = data['Combined'].tolist()\n",
        "\n",
        "skills_embeddings = get_embeddings(skills_text)\n",
        "tests_embeddings = get_embeddings(tests_text)\n",
        "\n",
        "test_names = data.apply(lambda row: f\"{row['Test Type']} - {row['Subcategory']}\", axis=1).tolist()\n",
        "\n",
        "\n",
        "st.title(\"Skill to Test Matching App\")\n",
        "\n",
        "\n",
        "skills_input = st.text_area(\"Enter your skill set (comma-separated, with weights like 'Skill1:1, Skill2:0.5'):\")\n",
        "skills = [(preprocess_text(skill.split(\":\")[0].strip()), float(skill.split(\":\")[1].strip())) for skill in skills_input.split(\",\") if \":\" in skill]\n",
        "\n",
        "if 'step' not in st.session_state:\n",
        "    st.session_state.step = 1\n",
        "\n",
        "if 'detailed_matches' not in st.session_state:\n",
        "    st.session_state.detailed_matches = []\n",
        "if 'matched_tests' not in st.session_state:\n",
        "    st.session_state.matched_tests = []\n",
        "if 'total_time' not in st.session_state:\n",
        "    st.session_state.total_time = 0\n",
        "if 'all_tests' not in st.session_state:\n",
        "    st.session_state.all_tests = []\n",
        "\n",
        "if st.session_state.step == 1:\n",
        "    if st.button(\"Find Tests\"):\n",
        "        skill_offset = 0\n",
        "        detailed_matches = match_skills_with_tests(skills, skill_offset, test_names, skills_embeddings, tests_embeddings)\n",
        "        matched_tests, total_time, all_tests = select_suitable_tests(skills, detailed_matches, data)\n",
        "\n",
        "        st.session_state.detailed_matches = detailed_matches\n",
        "        st.session_state.matched_tests = matched_tests\n",
        "        st.session_state.total_time = total_time\n",
        "        st.session_state.all_tests = all_tests\n",
        "        st.session_state.step = 2\n",
        "\n",
        "if st.session_state.step >= 2:\n",
        "    st.write(\"Recommended Tests:\")\n",
        "    for test, score in st.session_state.matched_tests:\n",
        "        st.write(f\"Test: {test} - Score: {score:.2f}\")\n",
        "    st.write(f\"Total Test Duration: {st.session_state.total_time} minutes\")\n",
        "\n",
        "    if st.session_state.step == 2:\n",
        "        change_num_tests = st.text_input(\"Do you want to change the number of tests? (yes/no):\")\n",
        "\n",
        "        if change_num_tests:\n",
        "            if change_num_tests.lower() == 'yes':\n",
        "                st.session_state.step = 3\n",
        "            elif change_num_tests.lower() == 'no':\n",
        "                st.write(\"DONE\")\n",
        "                test_names_to_save = [test[0] for test in st.session_state.matched_tests]\n",
        "                pd.DataFrame(test_names_to_save, columns=[\"Test Names\"]).to_csv(\"selected_tests.csv\", index=False)\n",
        "                st.session_state.step = 1\n",
        "            st.experimental_rerun()\n",
        "\n",
        "    elif st.session_state.step == 3:\n",
        "        adjust_choice = st.text_input(\"Do you want to increase or decrease the number of tests? (increase/decrease):\")\n",
        "\n",
        "        if adjust_choice:\n",
        "            if adjust_choice.lower() == 'increase':\n",
        "                current_tests = st.session_state.matched_tests\n",
        "                additional_test = [test for test in st.session_state.all_tests if test not in current_tests]\n",
        "                if additional_test:\n",
        "                    additional_test = additional_test[0]\n",
        "                    st.session_state.matched_tests.append(additional_test)\n",
        "                    st.session_state.total_time += data[data.apply(lambda row: f\"{row['Test Type']} - {row['Subcategory']}\", axis=1) == additional_test[0]]['Time'].values[0]\n",
        "\n",
        "                    st.write(\"Updated Recommended Tests:\")\n",
        "                    for test, score in st.session_state.matched_tests:\n",
        "                        st.write(f\"Test: {test} - Score: {score:.2f}\")\n",
        "\n",
        "                    st.write(f\"Updated Total Test Duration: {st.session_state.total_time} minutes\")\n",
        "\n",
        "            elif adjust_choice.lower() == 'decrease':\n",
        "                current_tests = st.session_state.matched_tests\n",
        "                similar_tests = find_similar_tests(current_tests, tests_embeddings)\n",
        "                test_to_remove, test_time = remove_lowest_score_test(current_tests, similar_tests, data)\n",
        "                st.session_state.matched_tests = [test for test in current_tests if test[0] != test_to_remove]\n",
        "                st.session_state.total_time -= test_time\n",
        "\n",
        "                st.write(\"Updated Recommended Tests:\")\n",
        "                for test, score in st.session_state.matched_tests:\n",
        "                    st.write(f\"Test: {test} - Score: {score:.2f}\")\n",
        "\n",
        "                st.write(f\"Updated Total Test Duration: {st.session_state.total_time} minutes\")\n",
        "\n",
        "\n",
        "            st.session_state.step = 2\n",
        "            st.experimental_rerun()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFf9Fvbzp5ji",
        "outputId": "6e8cc797-5367-4df8-84ff-69def83d5061"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.218.249:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.186s\n",
            "your url is: https://long-towns-tickle.loca.lt\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2024-06-27 20:18:40.711 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 589, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 158, in <module>\n",
            "    skills_embeddings = get_embeddings(skill_texts)\n",
            "  File \"/content/app.py\", line 38, in get_embeddings\n",
            "    inputs = tokenizer(text_list, return_tensors='pt', padding=True, truncation=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2883, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2969, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3160, in batch_encode_plus\n",
            "    return self._batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\", line 807, in _batch_encode_plus\n",
            "    batch_outputs = self._batch_prepare_for_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\", line 879, in _batch_prepare_for_model\n",
            "    batch_outputs = self.pad(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3299, in pad\n",
            "    raise ValueError(\n",
            "ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}